{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import BertTokenizer, BertForSequenceClassification, DataCollatorWithPadding\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport numpy as np\nfrom IPython.display import clear_output\nimport pandas as pd\n\nCONFIGS = [\n    {\"mode\": \"baseline\", \"use_scheduler\": False, \"early_stopping\": False},\n    {\"mode\": \"improved\", \"use_scheduler\": True, \"early_stopping\": True},\n    {\"mode\": \"extra_tuned\", \"use_scheduler\": True, \"early_stopping\": True, \"lr\": 3e-5, \"batch_size\": 32}\n]\n\nBASE_CONFIG = {\n    \"model_name\": \"bert-base-multilingual-cased\",\n    \"model_cache\": \"./hf_models_cache\",\n    \"dataset_cache\": \"./hf_datasets_cache\",\n    \"output_dir\": \"./outputs\",\n    \"selected_languages\": [\"en-US\", \"hi-IN\", \"es-ES\", \"fr-FR\"],\n    \"text_col\": \"utt\",\n    \"label_col\": \"intent\",\n    \"sample_frac\": 0.0001,\n    \"batch_size\": 64,\n    \"lr\": 5e-5,\n    \"epochs\": 5,\n    \"early_stopping_patience\": 2,\n    \"log_interval\": 1\n}\n\nos.makedirs(BASE_CONFIG[\"output_dir\"], exist_ok=True)\n\ndef save_label_info(label_list, save_dir, prefix=None, also_save_to_model_dir=False):\n    \"\"\"\n    Saves label list and index-to-label mapping as JSON files.\n\n    Args:\n        label_list (list): Ordered list of intent labels.\n        save_dir (str): Base directory to save the label files.\n        prefix (str or None): Prefix for filenames (e.g., \"full_massive\", \"filtered\", \"baseline\").\n                              If None, files will be named just 'label_list.json' and 'label_map.json'.\n        also_save_to_model_dir (bool): If True, saves 'label_list.json' to save_dir for inference use.\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n\n    # File name logic\n    list_filename = f\"label_list_{prefix}.json\" if prefix else \"label_list.json\"\n    map_filename = f\"label_map_{prefix}.json\" if prefix else \"label_map.json\"\n\n    # Save label list\n    list_path = os.path.join(save_dir, list_filename)\n    with open(list_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(label_list, f, indent=2, ensure_ascii=False)\n\n    # Save index-to-label map\n    label_map = {str(i): label for i, label in enumerate(label_list)}\n    map_path = os.path.join(save_dir, map_filename)\n    with open(map_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(label_map, f, indent=2, ensure_ascii=False)\n\n    # ‚úÖ Also save 'label_list.json' for inference if requested (in model folder)\n    if also_save_to_model_dir:\n        model_list_path = os.path.join(save_dir, \"label_list.json\")\n        with open(model_list_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(label_list, f, indent=2, ensure_ascii=False)\n\n    # ‚úÖ Log first few mappings for sanity check\n    print(f\"\\nüì¶ Saved {len(label_list)} labels to '{save_dir}':\")\n    print(f\"  üìÑ Label list file ‚Üí {list_path}\")\n    print(f\"  üìÑ Index‚Üílabel map ‚Üí {map_path}\")\n    if also_save_to_model_dir:\n        print(f\"  üß≠ Also saved 'label_list.json' for model loading ‚Üí {model_list_path}\")\n    print(\"üîç Sample mappings:\")\n    for i, label in enumerate(label_list[:5]):\n        print(f\"   {i}: {label}\")\n\n\nprint(\"üîÅ Loading and preprocessing dataset...\")\ntokenizer = BertTokenizer.from_pretrained(BASE_CONFIG[\"model_name\"], cache_dir=BASE_CONFIG[\"model_cache\"])\nraw_dataset = load_dataset(\"AmazonScience/massive\", \"all_1.1\", cache_dir=BASE_CONFIG[\"dataset_cache\"])\n\nsave_label_info(full_label_list, BASE_CONFIG[\"output_dir\"], prefix=\"full_massive\")\n\ndataset = DatasetDict({\n    split: raw_dataset[split].filter(lambda x: x[\"locale\"] in BASE_CONFIG[\"selected_languages\"])\n    for split in [\"train\", \"validation\", \"test\"]\n})\n\n\nsave_label_info(filtered_label_list, BASE_CONFIG[\"output_dir\"], prefix=\"filtered\")\n\n\nif BASE_CONFIG[\"sample_frac\"] < 1.0:\n    dataset = DatasetDict({\n        k: v.shuffle(seed=42).select(range(max(1, int(len(v) * BASE_CONFIG[\"sample_frac\"]))))\n        for k, v in dataset.items()\n    })\n\ndef tokenize(example):\n    return tokenizer(example[BASE_CONFIG[\"text_col\"]], truncation=True, padding=True)\n\ntokenized_dataset = dataset.map(tokenize, batched=True)\ntokenized_dataset = tokenized_dataset.rename_column(BASE_CONFIG[\"label_col\"], \"labels\")\ntokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\nlabel_list = tokenized_dataset[\"train\"].features[\"labels\"].names\nnum_labels = len(label_list)\n\nfinal_loss_logs = {}\nfinal_metrics = {}\n\nfor config in CONFIGS:\n    CONFIG = BASE_CONFIG.copy()\n    CONFIG.update(config)\n    print(f\"\\n\\nüîÑ Starting mode: {CONFIG['mode']}\")\n\n    data_collator = DataCollatorWithPadding(tokenizer)\n    train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=True, batch_size=CONFIG[\"batch_size\"], collate_fn=data_collator)\n    val_dataloader = DataLoader(tokenized_dataset[\"validation\"], batch_size=CONFIG[\"batch_size\"], collate_fn=data_collator)\n\n    model = BertForSequenceClassification.from_pretrained(CONFIG[\"model_name\"], cache_dir=CONFIG[\"model_cache\"], num_labels=num_labels)\n    optimizer = AdamW(model.parameters(), lr=CONFIG[\"lr\"])\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    steps, losses, cum_avgs = [], [], []\n    global_step = 0\n    best_val_loss = float(\"inf\")\n    patience_counter = 0\n    loss_log = []\n\n    for epoch in range(CONFIG[\"epochs\"]):\n        model.train()\n        total_train_loss = 0\n\n        for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Training [{CONFIG['mode']}]\", disable=True)):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n\n            total_train_loss += loss.item()\n            global_step += 1\n            steps.append(global_step)\n            losses.append(loss.item())\n            cum_avgs.append(sum(losses) / len(losses))\n\n            is_last = (epoch == CONFIG[\"epochs\"] - 1 and step == len(train_dataloader) - 1)\n            if (step + 1) % CONFIG[\"log_interval\"] == 0 and not is_last:\n                clear_output(wait=True)\n                plt.figure(figsize=(10, 4))\n                plt.plot(steps, losses, label='Step-wise Loss', color='blue')\n                plt.plot(steps, cum_avgs, label='Cumulative Avg Loss', color='orange', linestyle='--')\n                plt.title(f\"Training Loss - {CONFIG['mode']}\")\n                plt.xlabel(\"Global Step\")\n                plt.ylabel(\"Loss\")\n                plt.grid(True)\n                plt.legend()\n\n                cumulative_loss = sum(losses) / len(losses)\n\n                plt.text(0.01, 0.95, \n                         f\"Epoch {epoch+1}/{CONFIG['epochs']} | Step {step+1}/{len(train_dataloader)}\\n\"\n                         f\"Loss: {loss.item():.4f} | Cumulative Avg: {cumulative_loss:.4f}\",\n                         transform=plt.gca().transAxes, fontsize=10, verticalalignment='top',\n                         bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow'))\n\n                plt.tight_layout()\n                plt.show()\n\n        avg_train_loss = total_train_loss / len(train_dataloader)\n\n        model.eval()\n        total_val_loss, all_preds, all_labels = 0, [], []\n        with torch.no_grad():\n            for batch in val_dataloader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                outputs = model(**batch)\n                total_val_loss += outputs.loss.item()\n                preds = torch.argmax(outputs.logits, dim=1)\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(batch[\"labels\"].cpu().numpy())\n\n        avg_val_loss = total_val_loss / len(val_dataloader)\n        val_acc = accuracy_score(all_labels, all_preds)\n        val_prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n        val_rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n        val_f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n\n        loss_log.append({\n            \"epoch\": epoch + 1,\n            \"train_loss\": avg_train_loss,\n            \"val_loss\": avg_val_loss,\n            \"val_accuracy\": val_acc,\n            \"val_precision\": val_prec,\n            \"val_recall\": val_rec,\n            \"val_f1\": val_f1\n        })\n\n    # Save logs\n    final_loss_logs[CONFIG[\"mode\"]] = {\"steps\": steps, \"losses\": losses, \"cumulative\": cum_avgs}\n    final_metrics[CONFIG[\"mode\"]] = loss_log\n    with open(os.path.join(CONFIG[\"output_dir\"], f\"training_log_{CONFIG['mode']}.json\"), \"w\") as f:\n        json.dump(loss_log, f, indent=2)\n\n    # ‚úÖ Save final model after training\n    model_path = os.path.join(CONFIG[\"output_dir\"], f\"mbert_massive_finetuned_{CONFIG['mode']}\")\n    os.makedirs(model_path, exist_ok=True)\n    model.save_pretrained(model_path)\n    tokenizer.save_pretrained(model_path)\n    save_label_info(label_list, model_path, also_save_to_model_dir=True)\n\n\n    print(f\"\\n‚úÖ Final fine-tuned model saved for mode '{CONFIG['mode']}'\")\n    print(\"üìÅ Saved to:\", model_path)\n    print(\"üìÑ Files inside folder:\", os.listdir(model_path))\n\nclear_output(wait=True)\nprint(\"\\nüìà Final Training Loss Graphs\")\nfor mode, logs in final_loss_logs.items():\n    plt.figure(figsize=(10, 4))\n    plt.plot(logs[\"steps\"], logs[\"losses\"], label=\"Step-wise Loss\")\n    plt.plot(logs[\"steps\"], logs[\"cumulative\"], label=\"Cumulative Avg Loss\", linestyle=\"--\")\n    plt.title(f\"Final Training Loss - {mode}\")\n    plt.xlabel(\"Global Step\")\n    plt.ylabel(\"Loss\")\n    plt.grid(True)\n    plt.legend()\n    plt.tight_layout()\n    save_path = os.path.join(BASE_CONFIG[\"output_dir\"], f\"loss_plot_{mode}.png\")\n    plt.savefig(save_path)\n    print(f\"üì∏ Saved: {save_path}\")\n    plt.show()\n\ndef extract_metrics(log):\n    return {\n        \"epoch\": [e[\"epoch\"] for e in log],\n        \"train_loss\": [e[\"train_loss\"] for e in log],\n        \"val_loss\": [e[\"val_loss\"] for e in log],\n        \"val_accuracy\": [e[\"val_accuracy\"] for e in log],\n        \"val_precision\": [e[\"val_precision\"] for e in log],\n        \"val_recall\": [e[\"val_recall\"] for e in log],\n        \"val_f1\": [e[\"val_f1\"] for e in log]\n    }\n\nmodes = [cfg[\"mode\"] for cfg in CONFIGS]\nlogs = {mode: extract_metrics(final_metrics[mode]) for mode in modes}\n\nfig, axes = plt.subplots(3, 2, figsize=(14, 14))\nfig.suptitle(\"Training Metrics Across All Configs\", fontsize=18)\n\nmetric_keys = [\n    (\"train_loss\", \"Training Loss\"),\n    (\"val_loss\", \"Validation Loss\"),\n    (\"val_accuracy\", \"Validation Accuracy\"),\n    (\"val_precision\", \"Validation Precision\"),\n    (\"val_recall\", \"Validation Recall\"),\n    (\"val_f1\", \"Validation F1-Score\"),\n]\n\nfor ax, (key, title) in zip(axes.flat, metric_keys):\n    for mode in modes:\n        ax.plot(logs[mode][\"epoch\"], logs[mode][key], label=mode, marker='o')\n    ax.set_title(title)\n    ax.set_xlabel(\"Epoch\")\n    ax.set_ylabel(\"Score / Loss\")\n    ax.grid(True)\n    ax.legend()\n\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n\nsummary_rows = []\nfor mode, log in final_metrics.items():\n    last_epoch = log[-1]\n    summary_rows.append({\n        \"Mode\": mode,\n        \"Train Loss\": f\"{last_epoch['train_loss']:.4f}\",\n        \"Val Loss\": f\"{last_epoch['val_loss']:.4f}\",\n        \"Val Acc\": f\"{last_epoch['val_accuracy']:.4f}\",\n        \"Val Prec\": f\"{last_epoch['val_precision']:.4f}\",\n        \"Val Recall\": f\"{last_epoch['val_recall']:.4f}\",\n        \"Val F1\": f\"{last_epoch['val_f1']:.4f}\"\n    })\n\nsummary_df = pd.DataFrame(summary_rows)\nprint(\"\\nüìä Final Metrics Comparison Table:\")\ndisplay(summary_df)\n\nimport shutil\nimport zipfile\n\n# Paths\nsrc_dir = BASE_CONFIG[\"output_dir\"]\nfinal_dir = \"./final_package\"\nzip_name = \"mbert_final_package.zip\"\nzip_path = os.path.join(\"/kaggle/working\", zip_name)\n\n# Create final packaging folder\nif os.path.exists(final_dir):\n    shutil.rmtree(final_dir)\nos.makedirs(final_dir, exist_ok=True)\n\n# Move all files and folders from outputs to final_package\n#for item in os.listdir(src_dir):\n#    s = os.path.join(src_dir, item)\n#    d = os.path.join(final_dir, item)\n#    shutil.move(s, d)\n\nfor item in os.listdir(src_dir):\n    s = os.path.join(src_dir, item)\n    d = os.path.join(final_dir, item)\n    if os.path.isdir(s):\n        shutil.copytree(s, d)\n    else:\n        shutil.copy2(s, d)\n\n\nprint(f\"\\nüì¶ Moved all contents from {src_dir} ‚Üí {final_dir}\")\n\n# Zip the folder\nshutil.make_archive(base_name=zip_path.replace(\".zip\", \"\"), format=\"zip\", root_dir=final_dir)\nprint(f\"\\n‚úÖ Zipped folder created at: {zip_path}\")\nprint(\"üì• You can now download it from the sidebar under `/kaggle/working/`.\")\n\n#################################################################\n#  \n#################################################################\n\nimport os\n\n# Base output and cache directory\noutput_dir = BASE_CONFIG[\"output_dir\"]\nmodel_cache_dir = BASE_CONFIG[\"model_cache\"]\n\nprint(\"\\nüìÅ Checking saved fine-tuned models...\")\nfor config in CONFIGS:\n    mode = config[\"mode\"]\n    model_folder = os.path.join(output_dir, f\"mbert_massive_finetuned_{mode}\")\n    \n    if os.path.exists(model_folder):\n        print(f\"\\n‚úÖ Fine-tuned model for mode '{mode}' is saved at:\\n‚Üí {model_folder}\")\n        print(\"üìÑ Files inside:\")\n        for fname in os.listdir(model_folder):\n            print(\"  -\", fname)\n    else:\n        print(f\"\\n‚ùå Model folder for mode '{mode}' not found at:\\n‚Üí {model_folder}\")\n\n# ‚úÖ Also show pretrained model cache if available\npretrained_model_folder = os.path.join(model_cache_dir, \"models--bert-base-multilingual-cased\")\n\nif os.path.exists(pretrained_model_folder):\n    print(f\"\\nüì¶ Pretrained model is cached at:\\n‚Üí {pretrained_model_folder}\")\n    \n    print(\"\\nüìÑ Listing files inside pretrained model folder:\")\n    for root, dirs, files in os.walk(pretrained_model_folder):\n        for file in files:\n            full_path = os.path.join(root, file)\n            rel_path = os.path.relpath(full_path, pretrained_model_folder)\n            print(\"  -\", rel_path)\nelse:\n    print(f\"\\n‚ùå Pretrained model not found in cache dir: {pretrained_model_folder}\")\n\n#################################################################\n#  \n#################################################################\n\nimport os\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom datasets import load_dataset\n\n# ‚úÖ Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"üñ•Ô∏è  Using device: {device}\")\n\n# ‚úÖ Choose model to test\navailable_modes = [\"baseline\", \"improved\", \"extra_tuned\"]\nmode_to_test = \"improved\"  # üëà Change this as needed\n\nif mode_to_test not in available_modes:\n    raise ValueError(f\"Invalid mode '{mode_to_test}'. Choose from: {available_modes}\")\n\n# ‚úÖ Define paths\nmodel_dir = f\"./outputs/mbert_massive_finetuned_{mode_to_test}\"\nprint(f\"\\nüìÅ Loading model and tokenizer from: {model_dir}\")\n\nif not os.path.exists(model_dir):\n    raise FileNotFoundError(f\"‚ùå Model directory not found: {model_dir}\")\n\n# ‚úÖ Load tokenizer and model (local only)\ntokenizer = BertTokenizer.from_pretrained(model_dir, local_files_only=True)\nmodel = BertForSequenceClassification.from_pretrained(model_dir, local_files_only=True)\nmodel.to(device)\nprint(\"‚úÖ Model and tokenizer loaded successfully.\")\n\n# ‚úÖ Load label names (intent classes)\nprint(\"üîç Loading intent label classes from MASSIVE dataset...\")\nlabel_names = load_dataset(\"AmazonScience/massive\", \"all_1.1\")[\"train\"].features[\"intent\"].names\nclass_names = label_names\nprint(f\"‚úÖ Loaded {len(class_names)} intent classes.\")\n\n# ‚úÖ Define prediction function\ndef predict_intent(text, model, tokenizer, class_names):\n    model.eval()\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n        probs = torch.softmax(outputs.logits, dim=-1)\n        predicted_class = torch.argmax(probs, dim=1).item()\n        confidence = probs[0, predicted_class].item()\n    return class_names[predicted_class], confidence\n\n# ‚úÖ Test sentences\nsentences = {\n    \"greeting\": {\n        \"English\": \"How are you?\",\n        \"Spanish\": \"¬øC√≥mo est√°s?\",\n        \"French\": \"Comment √ßa va?\",\n        \"Hindi\": \"‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç?\"\n    },\n    \"weather_query\": {\n        \"English\": \"What's the weather like today?\",\n        \"Spanish\": \"¬øC√≥mo est√° el clima hoy?\",\n        \"French\": \"Quel temps fait-il aujourd'hui?\",\n        \"Hindi\": \"‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§ï‡•à‡§∏‡§æ ‡§π‡•à?\"\n    },\n    \"alarm_set\": {\n        \"English\": \"Set an alarm for 7 AM.\",\n        \"Spanish\": \"Configura una alarma para las 7 AM.\",\n        \"French\": \"Mets une alarme √† 7 heures.\",\n        \"Hindi\": \"‡§∏‡•Å‡§¨‡§π 7 ‡§¨‡§ú‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§≤‡§æ‡§∞‡•ç‡§Æ ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç‡•§\"\n    }\n}\n\n# ‚úÖ Run predictions\nprint(f\"\\nüîÆ Running predictions using model: '{mode_to_test}'\")\nfor intent_type, lang_dict in sentences.items():\n    print(f\"\\n=== üåç Intent: {intent_type.upper()} ===\")\n    for lang, sentence in lang_dict.items():\n        intent, confidence = predict_intent(sentence, model, tokenizer, class_names)\n        print(f\"[{lang}] ‚Üí \\\"{sentence}\\\"\")\n        print(f\"  ‚Ü™ Predicted: {intent} (Confidence: {confidence:.2f})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T07:37:42.990772Z","iopub.execute_input":"2025-06-21T07:37:42.991463Z","iopub.status.idle":"2025-06-21T07:37:43.036173Z","shell.execute_reply.started":"2025-06-21T07:37:42.991429Z","shell.execute_reply":"2025-06-21T07:37:43.035191Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_35/2274657562.py\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    ]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ']' does not match opening parenthesis '{' on line 16\n"],"ename":"SyntaxError","evalue":"closing parenthesis ']' does not match opening parenthesis '{' on line 16 (2274657562.py, line 19)","output_type":"error"}],"execution_count":1}]}