# ğŸ§  deep-data-squad-19: Cross-Lingual Intent Classification using BERT

**Mini Project by Team Data Miners**

## ğŸ“Œ Project Overview

Conversational AI systemsâ€”such as chatbots, virtual assistants, and automated customer supportâ€”must increasingly operate across languages. This project focuses on **Cross-Lingual Intent Classification** using **multilingual BERT (mBERT)** to classify user intents in multiple languages: **English, Spanish, French, and Hindi**.

We fine-tune a transformer-based model (BERT) to accurately detect user intent across language boundaries, making AI systems more inclusive and capable of seamless cross-cultural interaction.

---

## ğŸ¯ Objectives

- Classify user intents using pre-trained **mBERT**.
- Address challenges like:
  - Language diversity
  - Data sparsity
  - Domain-specific vocabulary
- Evaluate multilingual performance using:
  - Accuracy
  - Precision
  - Recall
  - F1-score

---

## ğŸ“‚ Repository Structure

```
deep-data-squad-19/
â”‚
â”œâ”€â”€ code/                                    
â”‚   â”œâ”€â”€ multilingual_intent_classification.ipynb # Main notebook
â”œâ”€â”€ requirements.txt                         	 # Python dependencies
â”œâ”€â”€ data/                                        # Folder for multilingual datasets
â”‚   â”œâ”€â”€ english.csv
â”‚   â”œâ”€â”€ spanish.csv
â”‚   â”œâ”€â”€ french.csv
â”‚   â””â”€â”€ hindi.csv
â”œâ”€â”€ reports/								    # Folder for evaluation reports
â”œâ”€â”€ models/        								# Folder for saved model checkpoints
â”‚   â””â”€â”€ mbart_model.pth                          # Fine-tuned mBERT model
â””â”€â”€ README.md                                   # Project documentation
```
---

## ğŸ”§ Installation

1. **Clone the repository**:

```bash
git clone https://github.com/your-username/deep-data-squad-19.git
cd deep-data-squad-19
```


2.	Create a virtual environment and install dependencies:
â€¢	On Linux/macOS:
	```bash
	python -m venv venv
	source venv/bin/activate  
	```
    â€¢	On Windows use 
	```bash
	venv\Scripts\activate
	```
3.	Install required dependencies:
	```bash
	pip install -r requirements.txt
	```

---

## ğŸš€ Usage

Launch Jupyter Notebook and run the classification pipeline:

jupyter notebook multilingual_intent_classification.ipynb

The notebook includes steps for:
	â€¢	Data loading and preprocessing
	â€¢	Tokenization using mBERT
	â€¢	Fine-tuning the model
	â€¢	Evaluating across all supported languages

Ensure your dataset files (english.csv, spanish.csv, etc.) are placed inside the data/ folder.


## ğŸ“Š Evaluation Metrics

We use the following performance metrics to evaluate the model for each language:
	â€¢	âœ… Accuracy
	â€¢	ğŸ“ Precision
	â€¢	ğŸ” Recall
	â€¢	ğŸ… F1-Score

This helps assess mBERTâ€™s cross-lingual understanding and intent detection capability.


## ğŸ§ª Dependencies

The project uses the following libraries:
	â€¢	transformers
	â€¢	torch
	â€¢	scikit-learn
	â€¢	pandas
	â€¢	numpy
	â€¢	matplotlib

Full list in: requirements.txt


## ğŸ¤ Contributors
	â€¢	Abhishek Gupta
	â€¢	Bharat Karthi R K
    â€¢	Gayathri ramasubramaniyam
	â€¢	Harikrishnan C
    â€¢	Indrerjit Singh Chahuan
	â€¢	Monika Tyagi



## ğŸ“œ License

This project is for educational and academic use. For any external or commercial usage, please contact the authors.



## ğŸŒ Acknowledgments

Special thanks to:
	â€¢	Hugging Face ğŸ¤— for the transformers library
	â€¢	BERT multilingual pre-trained models
	â€¢	Jupyter & open-source community

