# ğŸ§  deep-data-squad-19: Cross-Lingual Intent Classification using BERT

**Mini Project by Team Data Miners**

## ğŸŒ Live Demo

Try the multilingual intent classification demo here:  
ğŸ‘‰ [Hugging Face Spaces â€“ IntentBert](https://huggingface.co/spaces/charikri/IntentBert#multilingual-intent-classification)

---

## ğŸ“Œ Project Overview

Conversational AI systemsâ€”such as chatbots, virtual assistants, and automated customer supportâ€”must increasingly operate across languages. This project focuses on **Cross-Lingual Intent Classification** using **multilingual BERT (mBERT)** to classify user intents in multiple languages: **English, Spanish, French, and Hindi**.

We fine-tune a transformer-based model (BERT) to accurately detect user intent across language boundaries, making AI systems more inclusive and capable of seamless cross-cultural interaction.

---

## ğŸ¯ Objectives

- Classify user intents using pre-trained **mBERT**.
- Address challenges like:
  - Language diversity
  - Data sparsity
  - Domain-specific vocabulary
- Evaluate multilingual performance using:
  - Accuracy
  - Precision
  - Recall
  - F1-score

---

## ğŸ“š Dataset

This project leverages the **MASSIVE dataset** provided by Amazon Science:  
ğŸ‘‰ [Hugging Face â€“ MASSIVE Dataset](https://huggingface.co/datasets/AmazonScience/massive)

MASSIVE is a multilingual dataset for NLU tasks and supports over 50 intents across 51 languages, making it an ideal resource for multilingual intent classification.

---

## ğŸ“„ Reports and Documentation

- [Final Report (PDF)](https://github.com/monikatyagiisc/deep-data-squad-19/blob/main/reports/Final_Report.pdf)  
- [Shared Spreadsheet â€“ Team Documentation](https://indianinstituteofscience-my.sharepoint.com/:x:/g/personal/rgayathri_iisc_ac_in/ER1sFJFsbsFLj66COTgWDYoBy5lkDJKdJZZ77cQ8MIeHsg?e=UK7NUV&ovuser=6f15cd97-f6a7-41e3-b2c5-ad4193976476%2Cmonikatyagi%40iisc.ac.in&clickparams=eyJBcHBOYW1lIjoiVGVhbXMtRGVza3RvcCIsIkFwcFZlcnNpb24iOiI1MC8yNTA2MDIwNjYxMiIsIkhhc0ZlZGVyYXRlZFVzZXIiOmZhbHNlfQ%3D%3D)   
- Architecture diagram: `Documents/architecture.puml`  
- Training heatmap: `models/model_training_performance_heatmap.png`  
- Paper assets: `reports/DA-225o-Deep-Learning-Team-19/`

---

## ğŸ“‚ Repository Structure

```
deep-data-squad-19/
â”œâ”€â”€ code/
â”‚   â””â”€â”€ multilingual_intent_classification.ipynb          # Main notebook
â”œâ”€â”€ data/                                                 # Multilingual datasets
â”‚   â”œâ”€â”€ english.csv
â”‚   â”œâ”€â”€ french.csv
â”‚   â”œâ”€â”€ hindi.csv
â”‚   â”œâ”€â”€ spanish.csv
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ Documents/
â”‚   â””â”€â”€ architecture.puml                                 # UML architecture diagram
â”œâ”€â”€ models/                                               # Saved model checkpoints & performance visuals
â”‚   â”œâ”€â”€ mbart_model.pth
â”‚   â”œâ”€â”€ model_training_performance_heatmap.png
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ reports/                                              # Evaluation reports and LaTeX paper
â”‚   â”œâ”€â”€ Abstract.pdf
â”‚   â”œâ”€â”€ Final_Report.pdf
â”‚   â””â”€â”€ DA-225o-Deep-Learning-Team-19/
â”‚       â”œâ”€â”€ architecture.png
â”‚       â”œâ”€â”€ f1_chart.png
â”‚       â”œâ”€â”€ main.tex
â”‚       â”œâ”€â”€ mybibfile.bib
â”‚       â””â”€â”€ Tuning.png
â”œâ”€â”€ README.md                                             # Project documentation
â””â”€â”€ requiremets.txt                                       # Python dependencies
```

## ğŸ”§ Installation

1. **Clone the repository**:

```bash
git clone https://github.com/your-username/deep-data-squad-19.git
cd deep-data-squad-19
```


2.	Create a virtual environment and install dependencies:
â€¢	On Linux/macOS:
	```bash
	python -m venv venv
	source venv/bin/activate  
	```
    â€¢	On Windows use 
	```bash
	venv\Scripts\activate
	```
3.	Install required dependencies:
	```bash
	pip install -r requirements.txt
	```

---

## ğŸš€ Usage

Launch Jupyter Notebook and run the classification pipeline:

jupyter notebook multilingual_intent_classification.ipynb

The notebook includes steps for:
	â€¢	Data loading and preprocessing
	â€¢	Tokenization using mBERT
	â€¢	Fine-tuning the model
	â€¢	Evaluating across all supported languages

Ensure your dataset files (english.csv, spanish.csv, etc.) are placed inside the data/ folder.


## ğŸ“Š Evaluation Metrics

We use the following performance metrics to evaluate the model for each language:
	â€¢	âœ… Accuracy
	â€¢	ğŸ“ Precision
	â€¢	ğŸ” Recall
	â€¢	ğŸ… F1-Score

This helps assess mBERTâ€™s cross-lingual understanding and intent detection capability.


## ğŸ§ª Dependencies

The project uses the following libraries:
	â€¢	transformers
	â€¢	torch
	â€¢	scikit-learn
	â€¢	pandas
	â€¢	numpy
	â€¢	matplotlib

Full list in: requirements.txt


## ğŸ¤ Contributors
- **Abhishek Gupta** â€“ abhishekgup1@iisc.ac.in  
- **Bharat Karthi R K** â€“ barathkarth1@iisc.ac.in  
- **Gayathri Ramasubramanian** â€“ rgayathri@iisc.ac.in  
- **Harikrishnan C** â€“ charikrishna@iisc.ac.in  
- **Inderjit Singh Chauhan** â€“ inderjits@iisc.ac.in  
- **Monika Tyagi** â€“ monikatyagi@iisc.ac.in



## ğŸ“œ License

This project is for educational and academic use. For any external or commercial usage, please contact the authors.



## ğŸŒ Acknowledgments

Special thanks to:
	â€¢	Hugging Face ğŸ¤— for the transformers library
	â€¢	BERT multilingual pre-trained models
	â€¢	Jupyter & open-source community

